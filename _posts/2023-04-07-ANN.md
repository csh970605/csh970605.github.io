---
title: Artificial Neural Networks(ANN)
author: SeHoon
date: 2023-04-07 14:19:30 +0900
categories: [Deep Learning, DL_Introduction]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is ANN?
ANN is computing systems inspired by the biological neural networks that constitute animal brains.<br>
ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. <br>
You can see the example image of artificial neuron below.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230559902-ae339184-d056-4589-88cd-90b5f4f41a72.png" width=600>
</center>
<Br>
Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.<br>
Artificial neuron receives signals then processes them and can signal neurons connected to it.<br>
The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs.<br>
<br><br>



# How do Nueral Networks work?
Assuming that we are predicting the price of house and there are four input parameteres(area, bedrooms, distance to city, age).<br>
In the basic form of neural network has only input layer and output layer without hidden layer like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230567899-8d3caf76-2c6d-49b6-a5f3-9a897f12c846.png" width=400>
</center>
<br>

So, in this form, as for what the input variables do, they weighted up by the synapse, then output layer will be calculated by certain algorithm for example $(\text{price} = w1 \times x1 + w2 \times 2 + w3 \times 3 + w4 \times 4)$
<center>

<img src="https://user-images.githubusercontent.com/28240052/230567831-97f76039-2b73-47cf-b607-64b109522285.png" width=400>
</center>
<br>

But, if you do so, there are no flexibility and functionality compared to ann.<br>
Ann provides flexibility and functionality using hidden layers like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230569430-18f079ed-8df7-4a1d-9349-0436a68dd8e6.png" width=400>
</center>
<br>
Then, how does hidden layer provide flexibility and functionality?<br>
Let's assume that the first neuran in the hidden layer is affected by area and distance.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230569906-1b8358f6-a2f9-4afc-96c7-97928ac8d203.png" width=400>
</center>
<br>
Since all other neurons are also affected in a specific way the result will be:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230570273-b34d998b-9a0d-4672-9244-b9ae2663f2e9.png" width=400>
</center><br>
After that, each neuran in hidden layer will be calculated by certain algorithm like basic form of neural network does.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230570570-5765ce4a-c4fe-49fb-9576-69cad75d099a.png" width=400>
</center>
<br><br><br>


# How do Neural Networks learn?
To sum up, Neural Networks learn by adjusting the weights of nodes until the cost $C$ is significantly low.<br>
And the formula of cost $C$ is:
<center>
<font size=4>

$\frac{1 \times (\hat{y}-y)^{2}}{2}$
</font>
</center>


To explain the conclusion, suppose there is a neuron and the actual value y as shown in the image below.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230596634-17634957-e4fe-419d-862a-f3775b15bbee.png" width=600>
</center>
<br><br>

The first activation(forward propagation) through this neuron gives us the predicted value $\hat{y}$ and the cost $C$.<br>
And those values are:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230597027-9b0ddb3e-0390-4484-a7ef-eb190ccac3a9.png" width=400>
</center>
<br><br>

Since the $C$ is too high, we [backpropagate](https://csh970605.github.io/posts/Back_Propagation/) the informations to update the weights.<br>
By newly updated weights, we activate the neuron again and get the result like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230597436-7de02e7c-d41e-431e-9ae9-4111a9d7ca68.png" width=400>
</center>
<br><br>

We will repeat the above process until $C$ is significantly low.<br>
Although $C$ does not become zero in the normal case, we get:

<center>
<img src="https://user-images.githubusercontent.com/28240052/230597899-1706ef46-29da-47f1-8620-dace148b4a89.png" width=400>
</center>
<br><br>

A series of processes that reduce the value of $C$ above is called [Gradient Descent](https://csh970605.github.io/posts/Gradient_Descent/).

<br><br><br>


# Example
<br><br>

## Code
---
<br>

```py
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import confusion_matrix, accuracy_score

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# optimizer='adam' : parameter for stochastic gradient descent
# Loss function used in binary classification: binary_crossentropy
# Loss function used when non-binary classification: categorical_crossentropy
ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
ann.fit(X_train, y_train, batch_size=32, epochs=100)
y_pred = ann.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
```
<br><br>

## Result
---
<br>

```
0.863
```

<br><br><br><br>

# Implementation

+ [ANN 1](https://github.com/csh970605/Deep_Learning_A-Z/tree/main/Part%201%20-%20ANN/Section%202%20-%20Artificial%20Neural%20Networks%20(ANN))
+ [ANN 2](https://github.com/csh970605/Neural_Networks_in_Python)
+ [ANN 3](https://github.com/csh970605/Complete-Guide-on-TensorFlow-2.0/tree/main/Section%203)
+ [ANN 4](https://github.com/csh970605/TensorFlow-2.0-Practical/tree/main/Section%203)
+ [ANN 5](https://github.com/csh970605/TensorFlow-2.0-Practical/tree/main/Section%204)
+ [ANN for image classification](https://github.com/csh970605/Computer-Vision-Masterclass/tree/main/Section%204)
