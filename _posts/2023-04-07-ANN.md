---
title: Artificial Neural Networks (ANN)
author: SeHoon
date: 2023-04-07 14:19:30 +0900
categories: [Deep Learning, DL_Introduction]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is an ANN?
ANNs are computing systems inspired by the biological neural networks that constitute animal brains. ANNs are based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. You can see an example image of an artificial neuron below.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230559902-ae339184-d056-4589-88cd-90b5f4f41a72.png" width=600>
</center>
<Br>
Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals, processes them, and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by a non-linear function of the sum of its inputs.<br>
<br><br>



# How do Neural Networks Work?

Assuming that we are predicting the price of a house and there are four input parameters (area, bedrooms, distance to the city, age). The basic form of a neural network has only an input layer and an output layer without a hidden layer, as shown below:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230567899-8d3caf76-2c6d-49b6-a5f3-9a897f12c846.png" width=400>
</center>
<br>

 In this form, the input variables are weighted by the synapse, and the output layer is calculated by a certain algorithm. For example: $(\text{price} = w1 \times x1 + w2 \times x2 + w3 \times x3 + w4 \times x4)$
<center>

<img src="https://user-images.githubusercontent.com/28240052/230567831-97f76039-2b73-47cf-b607-64b109522285.png" width=400>
</center>
<br>

However, this form lacks the flexibility and functionality provided by ANNs. ANNs provide flexibility and functionality using hidden layers, as shown below:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230569430-18f079ed-8df7-4a1d-9349-0436a68dd8e6.png" width=400>
</center>
<br>
How do hidden layers provide flexibility and functionality? Let's assume that the first neuron in the hidden layer is affected by area and distance.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230569906-1b8358f6-a2f9-4afc-96c7-97928ac8d203.png" width=400>
</center>
<br>
Since all other neurons are also affected in a specific way the result will be:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230570273-b34d998b-9a0d-4672-9244-b9ae2663f2e9.png" width=400>
</center><br>
After that, each neuron in the hidden layer will be calculated by a certain algorithm, similar to the basic form of a neural network.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230570570-5765ce4a-c4fe-49fb-9576-69cad75d099a.png" width=400>
</center>
<br><br><br>


# How do Neural Networks Learn?
To sum up, neural networks learn by adjusting the weights of nodes until the cost $C$ is significantly reduced. The formula for cost $C$ is:
<center>
<font size=4>

$\frac{1 \times (\hat{y}-y)^{2}}{2}$
</font>
</center>


To explain this further, suppose there is a neuron and the actual value $y$ as shown in the image below.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230596634-17634957-e4fe-419d-862a-f3775b15bbee.png" width=600>
</center>
<br><br>

The first activation (forward propagation) through this neuron gives us the predicted value $\hat{y}$ and the cost $C$.<br>
And those values are:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230597027-9b0ddb3e-0390-4484-a7ef-eb190ccac3a9.png" width=400>
</center>
<br><br>

Since the cost $C$ is too high, we [backpropagate](https://csh970605.github.io/posts/Back_Propagation/) the information to update the weights.<br>
With the newly updated weights, we activate the neuron again and get the result like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230597436-7de02e7c-d41e-431e-9ae9-4111a9d7ca68.png" width=400>
</center>
<br><br>

We repeat the above process until the cost $C$ is significantly low. Although the cost $C$ does not become zero in normal cases, we get:

<center>
<img src="https://user-images.githubusercontent.com/28240052/230597899-1706ef46-29da-47f1-8620-dace148b4a89.png" width=400>
</center>
<br><br>

A series of processes that reduce the value of the cost $C$ above is called [Gradient Descent](https://csh970605.github.io/posts/Gradient_Descent/).

<br><br><br>


# Example
<br><br>

## Code
---
<br>

```py
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import confusion_matrix, accuracy_score

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# optimizer='adam' : parameter for stochastic gradient descent
# Loss function used in binary classification: binary_crossentropy
# Loss function used when non-binary classification: categorical_crossentropy
ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
ann.fit(X_train, y_train, batch_size=32, epochs=100)
y_pred = ann.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
```
<br><br>

## Result
---
<br>

```
0.863
```

<br><br><br><br>

# Implementation

+ [ANN 1](https://github.com/csh970605/Deep_Learning_A-Z/tree/main/Part%201%20-%20ANN/Section%202%20-%20Artificial%20Neural%20Networks%20(ANN))
+ [ANN 2](https://github.com/csh970605/Neural_Networks_in_Python)
+ [ANN 3](https://github.com/csh970605/Complete-Guide-on-TensorFlow-2.0/tree/main/Section%203)
+ [ANN 4](https://github.com/csh970605/TensorFlow-2.0-Practical/tree/main/Section%203)
+ [ANN 5](https://github.com/csh970605/TensorFlow-2.0-Practical/tree/main/Section%204)
+ [ANN for image classification](https://github.com/csh970605/Computer-Vision-Masterclass/tree/main/Section%204)
