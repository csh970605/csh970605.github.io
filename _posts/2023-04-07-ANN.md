---
title: Artificial Neural Networks(ANN)
author: SeHoon
date: 2023-04-07 14:19:30 +0900
categories: [Deep Learning, DL_Theory]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is ANN?
ANN is computing systems inspired by the biological neural networks that constitute animal brains.<br>
ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. <br>
You can see the example image of artificial neuron below.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230559902-ae339184-d056-4589-88cd-90b5f4f41a72.png" width=600>
</center>
<Br>
Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.<br>
Artificial neuron receives signals then processes them and can signal neurons connected to it.<br>
The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs.<br>
<br><br>

# What is Activation Function?
Activation function is used in 3rd step in the image below.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230564680-e8828b3e-fba4-4262-ab7e-5cf4ead2d0e9.png" width=600>
</center>
<br>
There are four types of activation function:<br>

+ **Threshold function**<br>
Threshold function returns 0 if the value is less than 0, and returns 1 if the value is more than 0.<br>
It's a type of yes/no function.<br>
To express it in a formula:
<center>
<font size=4>

$\phi(x)\ =\ \begin{cases} 1\ if\ x\geq0\\ 0\ if\ x < 0  \end{cases}$

</font>
</center>
&emsp; &emsp; &nbsp;And the graph will be:<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230561302-b0941deb-a5d0-4424-8fb9-d1d903b1b880.png" width=400>
</center>

+ **Sigmoid function**<br>
Sigmoid function is useful in the final layer(output layer) especially when predict probability<br>
To express it in a formula:
<center>
<font size=4>

$\phi(x)\ =\ \frac{1}{1\ +\ e^{-x}}$

</font>
</center>
&emsp; &emsp; &nbsp;And the graph will be:<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230562601-806d267d-1331-4c32-ab90-5f61f091f074.png" width=400>
</center>

+ **Rectifier function**<br>
Rectifier function is one of the most popular function for ANN.<br>
It returns 0 if the value is less than 0, and returns the value if the value is more than 0.<br>
To express it in a formula:
<center>
<font size=4>

$\phi(x)\ =\ max(x,\ 0)$

</font>
</center>
&emsp; &emsp; &nbsp;And the graph will be:<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230563301-2fb03fc1-8e1d-4377-a728-1007df2c4b07.png" width=400>
</center>

+ **Hyperbolic Tangent(tanh) function**<br>
It's smiliar to sigmoid function, but it goes below 0.<br>
To express it in a formula:
<center>
<font size=4>

$\phi(x)\ =\ \frac{1\ -\ e^{-2x}}{1\ +\ e^{-2x}}$

</font>
</center>
&emsp; &emsp; &nbsp;And the graph will be:<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230563995-11ed5afa-0936-4d5e-9442-b4561bea9df8.png" width=400>
</center>

# How do Nueral Networks work?
Assuming that we are predicting the price of house and there are four input parameteres(area, bedrooms, distance to city, age).<br>
In the basic form of neural network has only input layer and output layer without hidden layer like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230567899-8d3caf76-2c6d-49b6-a5f3-9a897f12c846.png" width=400>
</center>
<br>
So, in this form, as for what the input variables do, they weighted up by the synapse, then output layer will be calculated by certain algorithm for example (price = w1*x1 + w2*2 + w3*3 + w4*4)
<center>
<img src="https://user-images.githubusercontent.com/28240052/230567831-97f76039-2b73-47cf-b607-64b109522285.png" width=400>
</center>
<br>

But, if you do so, there are no flexibility and functionality compared to ann.<br>
Ann provides flexibility and functionality using hidden layers like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230569430-18f079ed-8df7-4a1d-9349-0436a68dd8e6.png" width=400>
</center>
<br>
Then, how does hidden layer provide flexibility and functionality?<br>
Let's assume that the first neuran in the hidden layer is affected by area and distance.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230569906-1b8358f6-a2f9-4afc-96c7-97928ac8d203.png" width=400>
</center>
<br>
Since all other neurons are also affected in a specific way the result will be:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230570273-b34d998b-9a0d-4672-9244-b9ae2663f2e9.png" width=400>
</center><br>
After that, each neuran in hidden layer will be calculated by certain algorithm like basic form of neural network does.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230570570-5765ce4a-c4fe-49fb-9576-69cad75d099a.png" width=400>
</center>
<br><br><br>


# How do Neural Networks learn?
To sum up, Neural Networks learn by adjusting the weights of nodes until the cost is significantly low.<br>
And the formula of cost is:
<center>
<font size=4>

$\frac{1*(\hat{y}-y)^{2}}{2}$
</font>
</center>


To explain the conclusion, suppose there is a neuron and the actual value y as shown in the image below.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230596634-17634957-e4fe-419d-862a-f3775b15bbee.png" width=600>
</center>
<br><br>

The first activation(forward propagation) through this neuron gives us the predicted value $\hat{y}$ and the cost *C*.<br>
And those values are:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230597027-9b0ddb3e-0390-4484-a7ef-eb190ccac3a9.png" width=400>
</center>
<br><br>

Since the *C* is too high, we backpropagate the informations to update the weights.<br>
By newly updated weights, we activate the neuron again and get the result like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230597436-7de02e7c-d41e-431e-9ae9-4111a9d7ca68.png" width=400>
</center>
<br><br>

We will repeat the above process until *C* is significantly low.<br>
Although *C* does not become zero in the normal case, we get:

<center>
<img src="https://user-images.githubusercontent.com/28240052/230597899-1706ef46-29da-47f1-8620-dace148b4a89.png" width=400>
</center>
<br><br><br>

# What is Gradient Descent?

Gradient Descent is the way to minimize *C*.<br>
You can see the graph of *C* below.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230599136-4db3b944-ab3c-4cc0-ad5e-e19102cb7af6.png" width=400>
</center>
<br><br>
And let's assume that the first *C* value is the red point.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230599103-70f4bf24-68cc-4a20-b977-cb97f45d3808.png" width=400>
</center>
<br><br>
We are going to get the gradient of *C* value. As you can see the red point is going downhill.<br>
After that, the redpoint will roll naturally and stop at:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230599968-8a84beb3-c6ef-4fd8-988e-a24116b0bcbb.png" width=400>
</center>
<br><br>
After repeat the process above until *C* is minimized like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230600129-ae25f494-7dfc-4687-94e0-dfb0e1ede561.png" width=400>
</center>
You will get well fitted weights.

<br><br>

## What is Stochastic Gradient Descent?
Stochastic Gradient Descent is to compensate for the disadvantage of Gradient Descent that cannot be implemented when there is not one convex place.<br>

Gradient Descent trains one dataset and then updates the weights, whereas Stochastic Gradient Descent updates the weights each time a row of the dataset is trained like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230601336-2eac18b2-38c8-4e11-9d1a-148c59037bfa.png">
</center>

<br><br><br>

# What is Backpropagation?
Update the weights according to how much they are responsible for the error like:.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230598392-c78537d5-2d95-4a15-ac56-f7487415f140.png" width=600>
</center>
<br><br>
The learning rate decides by how much we update the weights.<br>

# Example
<br><br>

## Code
---
<br>

```py
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from sklearn.metrics import confusion_matrix, accuracy_score

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

ann = tf.keras.models.Sequential()
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# optimizer='adam' : parameter for stochastic gradient descent
# Loss function used in binary classification: binary_crossentropy
# Loss function used when non-binary classification: categorical_crossentropy
ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
ann.fit(X_train, y_train, batch_size=32, epochs=100)
y_pred = ann.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
```
<br><br>

## Result
---
<br>

```
0.863
```