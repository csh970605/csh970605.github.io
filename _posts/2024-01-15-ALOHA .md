---
title: ALOHA
author: SeHoon
date: 2024-01-15 10:42:30 +0900
categories: [Papers, DeepLearning]
tags: [Papers, DeepLearning, ImitationLearning, AMRLabs]
math: true
mermaid: true
---

# ALOHA

ALOHA is an abbreviation of "A Low-cost Open-source HArdware system for bimanual teleoperation".<br>
As the name ALOHA suggests, it is used for fine manipulation tasks.<br>
In this [paper](https://arxiv.org/abs/2304.13705), the authors considered how robots can learn effectively at low cost. The answer is : they present a system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface.<br>
However, imitation learning has the problems which are errors in the policy can compound overtime and human demonstrations can be non-stationary.<br>
They present a method to overcome the problems in this paper. Then, let's see the detail.
<br><br><br><br>

# Architecture of ALOHA

To make the robot moves more precisely and reduce costs, they designed two systems:

+ [Teleoperation system](https://csh970605.github.io/posts/ALOHA/#teleoperation-system)
+ [Imitation learning algorithm](https://csh970605.github.io/posts/ALOHA/#imitation-learning-algorithm)<br>

<br><br>

## Teleoperation system

They devised a teleoperation setup with two sets of robot arms:<br>
<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/fed586ef-a474-44bf-94be-1ce9069c1345">
</center>
In the image above, two arms in the back are called "follower robot" and two arms in front are called "leader robot".<br>
As the name suggest, follower robots follows leader robots controlled by human like:
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/84d514b5-d612-45c7-8671-fb0f26c365b9"><br>

To make the robot aware of the situation, they set 4 cammeras, two on the top of the follower robots, one on the front and at the top respectively.

<br><br>

## Imitation learning algorithm
They found small errors in the predicted action can incur large differences in the state, exacerbating the compounding error problem of imitation learning.<br>
To tackle it, they used action chunking, a concept that describes how sequences of actions are grouped together as a chunk, and executed as one unit.<br>
In their case, the policy predicts the target joint positions for the next $k$ timesteps.<br>
Predicting action sequences also helps tackle temporally correlated confounders such as pauses in demonstrations that are hard to model with Markovian single-step policies.<br>
Also, to improve the smoothness of the policy, they propose temporal ensembling that queries the policy more frequently and averages across the overlapping action chunks:<br>
<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/3ce80b27-fc31-472f-bc3d-d14c3a1eeb02">
</center>

They implemented action chunking policy with transformers and train it as a CVAE(Conditional Variational Auto Encoder) to capture the variability in human data, and they named it ACT(Action Chunking with Transformers).

<br><br>

## ACT architecture