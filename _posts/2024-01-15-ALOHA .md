---
title: ALOHA
author: SeHoon
date: 2024-01-15 10:42:30 +0900
categories: [Papers, DeepLearning]
tags: [Papers, DeepLearning, ImitationLearning, AMRLabs]
math: true
mermaid: true
---

# ALOHA

ALOHA is an abbreviation of "A Low-cost Open-source HArdware system for bimanual teleoperation".<br>
As the name ALOHA suggests, it is used for fine manipulation tasks.<br>
In this [paper](https://arxiv.org/abs/2304.13705), the authors considered how robots can learn effectively at low cost. The answer is : they present a system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface.<br>
However, imitation learning has the problems which are errors in the policy can compound overtime and human demonstrations can be non-stationary.<br>
They present a method to overcome the problems in this paper. Then, let's see the detail.
<br><br><br><br>

# Architecture of ALOHA

To make the robot moves more precisely and reduce costs, they designed two systems:

+ [Teleoperation system](https://csh970605.github.io/posts/ALOHA/#teleoperation-system)
+ [Imitation learning algorithm](https://csh970605.github.io/posts/ALOHA/#imitation-learning-algorithm)<br>

<br><br>

## Teleoperation system

They devised a teleoperation setup with two sets of robot arms:<br>
<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/fed586ef-a474-44bf-94be-1ce9069c1345">
</center>
In the image above, two arms in the back are called "follower robot" and two arms in front are called "leader robot".<br>
As the name suggest, follower robots follows leader robots controlled by human like:
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/84d514b5-d612-45c7-8671-fb0f26c365b9"><br>

To make the robot aware of the situation, they set 4 cammeras, two on the top of the follower robots, one on the front and at the top respectively.

<br><br>

## Imitation learning algorithm
They found small errors in the predicted action can incur large differences in the state, exacerbating the compounding error problem of imitation learning.<br>
To tackle it, they used [action chunking](https://csh970605.github.io/posts/ALOHA/#action-chunk).<br>
Predicting action sequences also helps tackle temporally correlated confounders such as pauses in demonstrations that are hard to model with Markovian single-step policies.<br>
Also, to improve the smoothness of the policy, they proposed [temporal ensembling](https://csh970605.github.io/posts/ALOHA/#temporal-ensemble). 

They implemented action chunking policy with transformers and train it as a CVAE(Conditional Variational Auto Encoder) to capture the variability in human data, and they named it ACT(Action Chunking with Transformers).

<br><br>

# Action Chunk
Action chunk is a concept that describes how sequences of actions are grouped together as a chunk, and executed as one unit.<br>
In their case, the policy predicts the target joint positions for the next $k$ timesteps.<br>
It means, every $k$ steps, the agent receives an observation, generates the next $k$ actions, and executes the actions in sequence.<br>
This implies a $k$-fold reduction in the effective horizon of the task.<br>
Specifically, the policy model $\pi_{\theta}(a_{t:t+k}|s_{t})$ instead of $\pi_{\theta}(a_{t}|s_{t})$.<br>
That is, a single-step policy would struggle with temporraly correlated confounders, such as pauses in the middle of a demonstration, since the behavior not only depends on the state, but also the timestep.<br>
More detail, since $k$ dictates how long the sequence in each chunk is, we can analyze this hypothesis by varing $k$. If $k=1$ corresponds to no action chunking, and $k=episode_length$ corresponds to fully open-loop control, where the robot outputs the entire episode's action sequence based on the first observation.<br>
However, naive implementation of action chunking can be suboptimal because a new environment observation is incorporated abruptly every $k$ steps and can result in jerky robot motion.<br>
To improve smoothness and avoid discrete switching between executing and observing, they query the policy at every time step.<br>
Because of it, it makes different action chunks overlap with each other, and at a given timestep will be more than one predicted action.<br>
Since there are more than two different actions in single timestep, they proposed Temporal Ensembling.


<br><br><br><br>

# Temporal Ensemble

Temporal ensemble queries the policy more frequently and averages across the overlapping action chunks:<br>
<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/3ce80b27-fc31-472f-bc3d-d14c3a1eeb02">
</center>

Specifically, it performs a weighted average over those actions with an exponential weighting scheme $w_{i} = e^{-m*i}$ where $w_{0}$ is the oldest action and smaller $m$ is faster incorporation.<br>


<br><br><br><br>

# Human data Modeling

Human data modeling is a challenging task since it must learn from noisy human demonstrations. Thus, they made the policy focus on regions where high precision matters.<br>
They trained the policy as a CVAE to generate an action sequence conditioned on current observations.
<br><br><br><br>

# CVAE

CVAE is a conditional variational autoencoder.<br>
It consists of two components:

+ [CVAE encoder](https://csh970605.github.io/posts/ALOHA/#cvae-encoder)

+ [CVAE decoder](https://csh970605.github.io/posts/ALOHA/#cvae-decoder)

## CVAE encoder
CVAE encoder only serves to train the CVAE decoder and it is discarded at test.<br>
Specifically, the CVAE encoder predicts the mean and variance of the style variable$z$'s distribution, which is parameterized as a diagonal Gaussian, given the current obser vation and action sequence as inputs.<br>
Unlike standard CVAE, they leave out the image observations and only condition on the proprioceptive observation and the action sequence to make CVAE trained faster.

<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/eba3177a-10a8-4710-a242-b2ef9e23f2c2">
</center>

## CVAE decoder
CVAE decoder conditions on both $z$ and the current observations to predict the action sequence.<br>
The whole model is trained to maximize the log-likelihood of demonstration action chunks. The formula is :<br>
$min_{\theta} \ - \ \sum_{s_{t}, a_{t:t+k}\in D}log\pi_{\theta}(a_{t:T+k}|s_{t})$
<br>

<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/60771d27-ef5a-40be-8cb8-c54c2f37a1b6">
</center>

# ACT Architecture

There are two elements that consist of ACT:

+ [Training](https://csh970605.github.io/posts/ALOHA/#act-training)

+ [Testing](https://csh970605.github.io/posts/ALOHA/#act-testing)

<br><br><br><br>

## ACT Training

They record the joint positions of the leader robots(input from the human operator) by using ALOHA, and use them as actions.<br>
The observations are composed of the current joint positions of follower robots and the image feed from 4 cameras.(Step 1)<br>
Then, infer style variable $z$ using CVAE encoder.(Step 2)<br>
As you can see the image below, the weight matrix initialized randomly to vary the number of CLS tokens in the training stage.<br>
Next, they try to obtain the predicted action from CVAE decoder.(Step 3)<br>
For each of the image observations, it is first processed by a RESNet18 to obtain a feature map, and flattened to et a sequence of features.<br>
Those features are projected to the embedding dimesion with a linear layer, and added 2D sinusoidal position embedding to perserve the spatial information.<br>
The feature sequence from each camera is concatenated to be used as input to the transformer encoder. And there are two additional inputs are joint positions and $z$ which are also projected to the embedding dimension with two linear layers respectively.<br>
The output of the transformer encoder are used as "keys" and "values" in cross attention layers of the transformer decoder, which predicts action sequence given encoder output.<br>


**Intuitively, ACT treis to imitate what a human operator would do in the following time steps given current observations.**<br>

We can see the steps of training:

<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/54e26d00-5a58-4e44-92b5-bf54556f2ce3"><br>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/da446f57-65ac-44ef-b8f5-7fd424f84023"><br>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/f8e0ac7f-7b1a-4bc6-a4c2-ee8b9e82487e"><br>
</center>
<br><br>

Also, we can see the pseudocode:

<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/0cbc304d-38a9-405c-9093-5375efefc3fc">
</center>

<br><br><br><br>

## ACT Testing

At test time, CVAE decoder is only used as the policy. The incoming observations(images and joints) are fed into the model in the same ways as during training. The only difference is in $z$.<br>
They simply set $z$ to a 0 vector, which is the mean of the unit Gaussian prior used during training. So, given an observation, the output of the policy is always deterministic, benefiting policy evaluation.



We can see the processing pipelines of testing:
<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/e5bb7ca3-3a82-4fe9-ad2f-b3acb345dd58"><br>
</center>

Also, we can see the pseudocode:

<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/581a3661-7042-4897-851f-4b265a1120f1">
</center>


<br><br><br><br>

# Result

Through those methods, they made the robot arm working without manipulation. And also, it can modify the action if the task is failed.<br>
You can see the demonstration below.<br>
<br><br>

## Slot Battery

<center>

<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/b16d1822-4a42-468b-8760-ea538ab513a6">

</center>

<br><br>

## Open Lid

<center>

<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/8e332b7e-8a76-4145-9cc5-94e797a4dfd0">

</center>

<br><br>

## Attaching tape

<center>

<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/a06c41a8-4891-4ea5-89e6-eff29f121086">

</center>

<br><br>

## Put on shoes

<center>

<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/b1baeb54-0545-4fec-a364-3167fe942329">

</center>

<br><br>

## Reacting when the cup falls

<center>

<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/5cfad4af-bf28-45ee-af59-3440090ee19c">

</center>

<br><br>

## Reacting to a moving target

<center>

<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/2b576bd7-d706-4d1d-a6db-1667ef2a3fa7">

</center>