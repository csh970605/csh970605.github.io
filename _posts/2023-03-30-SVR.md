---
title: Support Vector for Regression(SVR)
author: SeHoon
date: 2023-03-30 20:33:00 +0900
categories: [Machine Learning, ML_Theory]
tags: [machine learning, python]
math: true
mermaid: true
---

# What is SVR?

Unlike simple linear regression, you can see a tube instead of a simple line. And there is a regression line in the tube.<br>
The tube is called the "epsilon insensitive Tube" which has a width of epsilon.<br>
Any points in the dataset that are inside the tube will be disregarded their error. In short, **think of the tube as the margin of error the model can have, and don't care about the error inside the tube.**<br>
<br>
But at the same time, we have points that are outside the tube.<br>
We treat them as errors, measured as the distance between each point and the tube. And the distances have names as you can see in the image below.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/229114533-ddb76abd-9259-4722-a829-38768f2c06ff.png" width = 500>
</center>
<br>
By the distances we can make formula:
<br><br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/229118208-0c37fa21-e0b1-4c96-8902-fea2553cca76.png" width = 400>
</center>

# Example
<br><br>

## Code
---
<br>

```py 
from sklearn.svm import SVR
regressor = SVR(kernel='rbf')#rbf = function to use as kernel in svm
sc_y = StandardScaler()
y = sc_y.fit_transform(y)
regressor.fit(X, y)
sc_y.inverse_transform(regressor.predict(X).reshape(-1, 1))
```
<br><br>

## Result
---
<br><br>

<center>
<img src="https://user-images.githubusercontent.com/28240052/229119168-2722bdf7-a06d-4a29-8ed6-54fc515c10bb.png" width=500>
</center>