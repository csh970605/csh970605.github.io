---
title: Support Vector for Regression (SVR)
author: SeHoon
date: 2023-03-30 20:33:00 +0900
categories: [Machine Learning, ML_Theory]
tags: [machine learning, python]
math: true
mermaid: true
---

# What is SVR?

Unlike simple linear regression, you can see a tube instead of a simple line, with a regression line inside the tube.<br>
The tube is called the "epsilon-insensitive tube" and has a width of epsilon.<br>

Any points in the dataset that are inside the tube will have their error disregarded. In short, **think of the tube as the margin of error the model can have, and don't care about the error inside the tube.**<br>
<br>
These points are treated as errors, measured as the distance between each point and the tube. These distances have names, as shown in the image below.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/229114533-ddb76abd-9259-4722-a829-38768f2c06ff.png" width = 500>
</center>
<br>

Using these distances, we can derive the following formula:
<br><br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/229118208-0c37fa21-e0b1-4c96-8902-fea2553cca76.png" width = 400>
</center>

# Example
<br><br>

## Code
---
<br>

```py 
from sklearn.svm import SVR
regressor = SVR(kernel='rbf')#rbf = function to use as kernel in svm
sc_y = StandardScaler()
y = sc_y.fit_transform(y)
regressor.fit(X, y)
sc_y.inverse_transform(regressor.predict(X).reshape(-1, 1))
```
<br><br>

## Result
---
<br><br>

<center>
<img src="https://user-images.githubusercontent.com/28240052/229119168-2722bdf7-a06d-4a29-8ed6-54fc515c10bb.png" width=500>
</center>

<br><br><br><br>

# Implementation

+ [SVR](https://github.com/csh970605/Machine-LearningA-Z/tree/main/Part%202%20-%20Regression/Section%207%20-%20Support%20Vector%20Regression%20(SVR)/python)