---
title: Recurrent Neural Network(RNN)
author: SeHoon
date: 2023-04-07 14:21:30 +0900
categories: [Deep Learning, DL_Introduction]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is RNN?
RNN is a type of supervised deep learning used for time series analysis. The basic structure of RNN looks like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230857618-46b78468-c68d-48cd-b531-45a97346902a.png" width=600>
</center>
<br><br>
The hidden layers (the blue circles) not only give output but also feed back into themselves.<br>

So, how does RNN update weights? Like ANN, RNN uses [backpropagation](https://csh970605.github.io/posts/Back_Propagation/).
<center>
<img src="https://user-images.githubusercontent.com/28240052/230866777-40b40891-0ecb-4a71-959b-f27959ad78f6.png" width=500>
</center>
<br><br>

However, there are some problems when updating weights ($W_{rec}$).<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230904926-14398da5-af23-4288-80f4-41041fbf7d2c.png">
</center>

Because the weights $W_{rec}$ are usually less than 1, it takes many epochs to update the weights in the back. And also, as the weight goes to the backward input, the effect on the output becomes smaller.<br>
This problem is called **vanishing gradient problem**. Conversely, if the weights $W_{rec}$ are more than 1, it is called **exploding gradient problem.**.<br>
Below, I will introduce ways to solve the vanishing and exploding gradient problems.
<br><br>

## Structure of various RNNs
---
<br>

+ One to Many<br>
It can be used to generate sentences from a picture.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230858549-36b2ce42-bc8a-4773-bb95-db13022c7702.png" width=400>
</center>
<br><br>

+ Many to One<br>
It can be used to classify whether the sentence is positive or negative.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230858729-5c1ec588-39be-49ff-af59-c335cea3adc2.png" width=400>
</center>
<br><br>

+ Many to Many<br>
It can be used for translation.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230859533-44387c7d-33e5-42f7-8173-d66656591908.png" width=400>
</center>

# Ways to solve vanishing and exploding
<br>
<br>

## Vanishing

There are three ways to solve vanishing<br>

+ Weight initialization<br>
+ Echo state networks<br>
+ [Long Short-Term Memory Networks(LSTM)](https://csh970605.github.io/posts/LSTM/)

<br><br>

## Exploding

+ Truncated Backpropagation<br>
+ Penalties<br>
+ Gradient Clipping<br>
<br><br>
<br>






