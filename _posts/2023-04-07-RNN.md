---
title: Recurrent Neural Network(RNN)
author: SeHoon
date: 2023-04-07 14:21:30 +0900
categories: [Deep Learning, DL_Introduction]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is RNN?
RNN is one of the the supervised deeplearning and used for time series analysis.<br>

The basic structure of RNN looks like:
<center>
<img src="https://user-images.githubusercontent.com/28240052/230857618-46b78468-c68d-48cd-b531-45a97346902a.png" width=600>
</center>
<br><br>
The hidden layers(the blue circles) not only give output but also feed back itself.<br>

Then, how does RNN update weights? Same as ANN, RNN use [backpropagation](https://csh970605.github.io/posts/Back_Propagation/).
<center>
<img src="https://user-images.githubusercontent.com/28240052/230866777-40b40891-0ecb-4a71-959b-f27959ad78f6.png" width=500>
</center>
<br><br>

But, there are some problems while update weights($W_{rec}$).<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/230904926-14398da5-af23-4288-80f4-41041fbf7d2c.png">
</center>

Because the weights($W_{rec}$) are usually less than 1, it takes many epochs to update the weights in the back. And also, as the weight goes to the backward input, the effect on the output becomes smaller.<br>
This problem is called **vanishing**.<br>
On the other way, if the weights($W_{rec}$) are more than 1, it is called **exploding**.<br>
I will introduce the way to solve vanishing and exploding below.
<br><br>

## Structure of various RNNs
---
<br>

+ One to Many<br>
It can be used to make sentences with a picture.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230858549-36b2ce42-bc8a-4773-bb95-db13022c7702.png" width=400>
</center>
<br><br>

+ Many to One<br>
It can be used to classify whether the setence is positive or not.
<center>
<img src="https://user-images.githubusercontent.com/28240052/230858729-5c1ec588-39be-49ff-af59-c335cea3adc2.png" width=400>
</center>
<br><br>

+ Many to Many<br>
It is used to translate
<center>
<img src="https://user-images.githubusercontent.com/28240052/230859533-44387c7d-33e5-42f7-8173-d66656591908.png" width=400>
</center>

# Ways to solve vanishing and exploding
<br>
<br>

## Vanishing

There are three ways to solve vanishing<br>

+ Weight initialization<br>
+ Echo state networks<br>
+ [Long Short-Term Memory Networks(LSTM)](https://csh970605.github.io/posts/LSTM/)

<br><br>

## Exploding

+ Truncated Backpropagation<br>
+ Penalties<br>
+ Gradient Clipping<br>
<br><br>
<br>






# Exmaple
<br><br>

## Code
---
<br>

```py
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

sc = MinMaxScaler(feature_range=(0, 1))
training_set_scaled = sc.fit_transform(training_set)
""" number of timesteps : 
    A data structure that specifies what the RNN should remember when predicting the next stock price.
    n timestep: last ~ last - n data
     """
X_train = []
y_train = []
for i in range(60, len(training_set)):
    X_train.append(training_set_scaled[i-60 : i, 0])
    y_train.append(training_set_scaled[i, 0])

X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

regressor = Sequential() # Regression is used because it predicts continuous values.
"""units : the number of neurons the LSTM will have in each layer
   return_sequence: When adding another layer after the LSTM layer: true / false if not added
   input _hape : timesteps(X_train.shape[1])
   rate: number of neurons to drop per layer"""
regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
regressor.add(Dropout(rate=0.2))
# Reason for not specifying input_shape : Because input_shape is provided in the first LSTM layer
regressor.add(LSTM(units=50, return_sequences=True)) 
regressor.add(Dropout(rate=0.2))
regressor.add(LSTM(units=50, return_sequences=True))
regressor.add(Dropout(rate=0.2))
regressor.add(LSTM(units=50))
regressor.add(Dropout(rate=0.2))
regressor.add(Dense(units=1))
regressor.compile(optimizer='adam', loss='mean_squared_error')

regressor.fit(X_train, y_train, epochs=100, batch_size=32)

predicted_stock_price = regressor.predict(X_test)
```



<br><br>

## Result
---
<br>

<center>
<img src="https://user-images.githubusercontent.com/28240052/231393705-779ac598-569d-425f-8d4a-e36f37b11482.png" width=500>
</center>

<br><br><br>

You can see my implementation on my [Github](https://github.com/csh970605/Deep_Learning_A-Z/tree/main/Part%203%20-%20RNN)