---
title: Softmax
author: SeHoon
date: 2023-04-17 16:00:00 +0900
categories: [Deep Learning, DL_Theory]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is Softmax?
Softmax function makes the predicted value between 0 and 1 so that the sum of all predicted values are 1.<br>
For example, there is [CNN](https://csh970605.github.io/posts/CNN/) and predicted values.
<center>
<img src="https://user-images.githubusercontent.com/28240052/232415817-ba6f97d6-a028-470b-8aa8-6accafd22708.png" width=500>
</center>
<br><br>
As you see in the image above, the sum of two predicted values are 1. Even the two output nodes aren't connected, how the sum of two predicted values are 1?<br>
It's because we apply "softmax" at the end synapses of CNN. The formula of softmax is:
<center>

$f_{j}(z)\ =\ \frac{e^{zj}}{\Sigma_{k}e^{zk}} $
</center>

Also, [cross entropy](https://csh970605.github.io/posts/Cost_function/) is used when calculating the error rate.