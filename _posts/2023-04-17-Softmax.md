---
title: Softmax
author: SeHoon
date: 2023-04-17 16:00:00 +0900
categories: [Deep Learning, DL_Theory]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is Softmax?
The Softmax function converts predicted values to be between 0 and 1, ensuring that the sum of all predicted values is 1. For example, consider a [CNN](https://csh970605.github.io/posts/CNN/) with predicted values.
<center>
<img src="https://user-images.githubusercontent.com/28240052/232415817-ba6f97d6-a028-470b-8aa8-6accafd22708.png" width=500>
</center>
<br><br>
As you see in the image above, the sum of the two predicted values is 1. Even though the two output nodes aren't connected, how is the sum of the two predicted values 1?" It's because we apply the Softmax function at the final layer of the CNN. The formula for the Softmax function is:
<center>

$f_{j}(z)\ =\ \frac{e^{zj}}{\Sigma_{k}e^{zk}} $
</center>

Additionally, [cross-entropy](https://csh970605.github.io/posts/Cost_function/) is used when calculating the error rate.