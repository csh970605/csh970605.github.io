---
title: Long Short-Term Memory Networks(LSTM)
author: SeHoon
date: 2023-04-10 15:21:30 +0900
categories: [Deep Learning, DL_Theory]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is LSTM?
LSTM is devised to solve the [vanishing gradient problem](https://csh970605.github.io/posts/RNN/#vanishing) in [RNN](https://csh970605.github.io/post/RNN). Here is a basic structural image of LSTM:
<center>
<img src="https://user-images.githubusercontent.com/28240052/231356494-cc304dd2-9ce2-475e-9f4f-9e5577e633d3.png" width=600>
</center>
<br><br>
The meanings of the symbols are shown below:
<center>
<img src="https://user-images.githubusercontent.com/28240052/231358828-960c55c4-df17-44d2-822f-57bbbf9f88d3.png" width=500>
</center>
<br>

One thing to notice is that $W_{rec}$ is fixed at 1. In LSTM, the top side pipeline, called the **memory cell**, represents $W_{rec}$.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/231357682-3774c0dc-735e-40cb-ae7b-eaca0bfcf64c.png" width=300>
</center>

<br>

In the memory cell, information can freely flow through time. Sometimes it might be erased in the $\times$ circle or added in the $+$ circle. <br>
To make it easier to understand, let's assume that we are translating. In the cell, the information about a boy goes through.

<center>
<img src="https://user-images.githubusercontent.com/28240052/231385380-b7d7562b-8195-4e47-89a7-5f51240a0791.png" width=300>
</center>
<br>
<br>

The information about the boy goes through until there is a new input in $X_{t}$. So, let's assume 'girl' is entered as a new input in $X_{t}$.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231386678-8cfd5ffc-bb5f-4b9e-bbf1-30238806117b.png" width=300>
</center>
<br><br>
Then, LSTM orders open and close some "pointwise operations".
<center>
<img src="https://user-images.githubusercontent.com/28240052/231387130-9826b390-f067-4788-b0c6-e441005a4f69.png" width=300>
</center>
<br><br>

By doing this, the memory for the boy is erased, and we have a new memory for the girl. In the cell, certain information is extracted, and this information will be the input for the next cell.

<center>
<img src="https://user-images.githubusercontent.com/28240052/231388259-ea92599c-a3b8-43f3-9cb3-771f74998a07.png" width=300>
</center>
<br><br><br>

## Variants of LSTM
---
<br>

+ Adding **peephole connections**<br>
It adds some lines to provide additional information about the current state of the memory cell to the sigmoid activation function layers.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388669-d1aef15c-b17e-4ed6-a7eb-92b996c2f0de.png" width=400>
</center>
<br><br>


+ Use coupled **forget and input gates**<br>
These gates interact so that when one is 1, the other is 0. In other words, nothing is added to the memory cell, keeping the value of the memory cell constant.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388740-6f4cc77d-78d4-4586-a223-2286fa856e71.png" width=400>
</center>

+ Gated Recurrent Unit (**GRU**)<br>
There is no memory cell; instead, it is replaced by a hidden pipeline.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388806-50d13a88-9b88-4622-b542-412a166cc93a.png" width=400>
</center>
<br><br><br>

# Example
<br><br>

## Code
---
<br>

```py
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

sc = MinMaxScaler(feature_range=(0, 1))
training_set_scaled = sc.fit_transform(training_set)
""" number of timesteps : 
    A data structure that specifies what the RNN should remember when predicting the next stock price.
    n timestep: last ~ last - n data
     """
X_train = []
y_train = []
for i in range(60, len(training_set)):
    X_train.append(training_set_scaled[i-60 : i, 0])
    y_train.append(training_set_scaled[i, 0])

X_train, y_train = np.array(X_train), np.array(y_train)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

regressor = Sequential() # Regression is used because it predicts continuous values.
"""units : the number of neurons the LSTM will have in each layer
   return_sequence: When adding another layer after the LSTM layer: true / false if not added
   input _hape : timesteps(X_train.shape[1])
   rate: The number of neurons to drop per layer"""
regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
regressor.add(Dropout(rate=0.2))
# Reason for not specifying input_shape : Because input_shape is provided in the first LSTM layer
regressor.add(LSTM(units=50, return_sequences=True)) 
regressor.add(Dropout(rate=0.2))
regressor.add(LSTM(units=50, return_sequences=True))
regressor.add(Dropout(rate=0.2))
regressor.add(LSTM(units=50))
regressor.add(Dropout(rate=0.2))
regressor.add(Dense(units=1))
regressor.compile(optimizer='adam', loss='mean_squared_error')

regressor.fit(X_train, y_train, epochs=100, batch_size=32)

predicted_stock_price = regressor.predict(X_test)
```



<br><br>

## Result
---
<br>

<center>
<img src="https://user-images.githubusercontent.com/28240052/231393705-779ac598-569d-425f-8d4a-e36f37b11482.png" width=500>
</center>

<br><br><br><br>

# Implementation

+ [LSTM 1](https://github.com/csh970605/Deep_Learning_A-Z/tree/main/Part%203%20-%20RNN)
+ [LSTM 2](https://github.com/csh970605/Complete-Guide-on-TensorFlow-2.0/tree/main/Section%205)
+ [LSTM 3](https://github.com/csh970605/TensorFlow-2.0-Advanced/blob/main/Section%206/LSTM_for_Shakespeare_like_Text_Generation!.ipynb)