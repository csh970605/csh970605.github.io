---
title: Long Short-Term Memory Networks(LSTM)
author: SeHoon
date: 2023-04-10 15:21:30 +0900
categories: [Deep Learning, DL_Theory]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is LSTM?
LSTM is divised to solve the vanishing problem in [RNN](https://csh970605.github.io/post/RNN)<br>
Here are basic structure image of LSTM:
<center>
<img src="https://user-images.githubusercontent.com/28240052/231356494-cc304dd2-9ce2-475e-9f4f-9e5577e633d3.png" width=600>
</center>
<br><br>
And you can see the meaning of sign below:
<center>
<img src="https://user-images.githubusercontent.com/28240052/231358828-960c55c4-df17-44d2-822f-57bbbf9f88d3.png" width=500>
</center>
<br>

There are one thing to notice that $W_{rec}$ is fixed as 1. And in LSTM, the top side pipeline called **memory cell** represents $W_{rec}$.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/231357682-3774c0dc-735e-40cb-ae7b-eaca0bfcf64c.png" width=300>
</center>

<br>

In memory cell, it can freely flow through time. Sometimes it might be erased in "X" circle or added in "+"circle.<br>
<br>
Easy to understand, let's assume that we are translating.<br>
And in the cell, the information of boy goes through.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231385380-b7d7562b-8195-4e47-89a7-5f51240a0791.png" width=300>
</center>
<br>
<br>

The information of boy goes through until there is a new input in $X_{t}$.<br>
So, let's "girl" is entered as a new input in $X_{t}$
<center>
<img src="https://user-images.githubusercontent.com/28240052/231386678-8cfd5ffc-bb5f-4b9e-bbf1-30238806117b.png" width=300>
</center>
<br><br>
Then, LSTM order open and close some "pointwise operations"
<center>
<img src="https://user-images.githubusercontent.com/28240052/231387130-9826b390-f067-4788-b0c6-e441005a4f69.png" width=300>
</center>
<br><br>
By doing this, the memory for the boy is erased and we have a new memory for the "girl".<br>
Then, in the cell, certain informations are extracted and the informations will be input of next cell.<br>
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388259-ea92599c-a3b8-43f3-9cb3-771f74998a07.png" width=300>
</center>
<br><br><br>

## Variant form of LSTM
---
<br>

+ Adding **peephole connections**.<br>
It adds some lines to provide additional informations about current state of memory cell to sigmoid activation function layers.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388669-d1aef15c-b17e-4ed6-a7eb-92b996c2f0de.png" width=400>
</center>
<br><br>


+ Use coupled **forget and input gates**
These gates interact so that when one is 1, the other is 0. In other words, nothing is added to the memory cell to keep the value of the memory cell going through.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388740-6f4cc77d-78d4-4586-a223-2286fa856e71.png" width=400>
</center>

+ Gated Recurrent Unit(**GRU**)
There is no memory cell and replace it as hidden pipeline.
<center>
<img src="https://user-images.githubusercontent.com/28240052/231388806-50d13a88-9b88-4622-b542-412a166cc93a.png" width=400>
</center>

