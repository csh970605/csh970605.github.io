---
title: Transfer Learning & Fine Tuning
author: SeHoon
date: 2023-05-04 22:05:30 +0900
categories: [Deep Learning, DL_Introduction]
tags: [deep learning, python]
math: true
mermaid: true
---

# What is Transfer Learning?
Transfer learning is a machine learning method where a model developed for a task is reused(pre trained model) as the starting point for a model on a second task.<br>
Pre trained model that called **base model** is used by delete a few last layers. After that, if we want to build [CNN](https://csh970605.github.io/posts/CNN/), we can build it just connecting the top part of CNN called **head** and the frozen base model.

<center>
<img src="https://user-images.githubusercontent.com/28240052/236216636-03ed8cea-528a-424b-9e09-a32fb7602c0e.png" width=500>
</center>
<br><br>
<br>
<br>

# What is Fine Tuning?
Fine tuning is more suited for bigger data sets somtimes when the custom data set is pretty different from the image and data set.<br>
Fine tuning is similar to Transfer Learning except the base model is half frozen.

<center>
<img src="https://user-images.githubusercontent.com/28240052/236217603-d13caea3-e0ec-425b-95dd-921b567788d7.png" width=500>
</center>
<br><br>
<br><br>

# When to use these techniques

| dataset | technique |
|---|---|
|Large and different|Train the whole model|
|Large and similar|Fine tuning|
|Small and different|Fine tuning|
|Small and similar|Transfer Learning|

<br><br><br><br>

# Example
<br><br>

## Code
---
<br>

**Transfer Learning**
```py
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SHAPE = (128, 128, 3)
# include_top : decide whether fully connected network or not
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')

# Freezing the base model
base_model.trainable = False

# because in this case, I would have 4*4*1280(20480) new weights to train. So, to reduce input shape I use GlobalAveragePooling
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
prediction_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(global_average_layer)

# combine two networks(base_model, prediction_layer)
model = tf.keras.models.Model(inputs=base_model.input, outputs=prediction_layer)

model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001), 
              loss='binary_crossentropy', 
              metrics=['accuracy'])
     
data_gen_train = ImageDataGenerator(rescale=1/255.)
data_gen_valid = ImageDataGenerator(rescale=1/255.)

train_generator = data_gen_train.flow_from_directory(train_dir, target_size=(128,128), batch_size=128, class_mode='binary')
valid_generator = data_gen_valid.flow_from_directory(validation_dir, target_size=(128,128), batch_size=128, class_mode='binary')

model.fit_generator(train_generator, epochs=50, validation_data=valid_generator)

valid_loss, valid_accuracy = model.evaluate_generator(valid_generator)
print("Accuracy after transfer learning: {}".format(valid_accuracy))
```
<br><br>

**Fine tuning**
```py
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SHAPE = (128, 128, 3)
# include_top : decide whether fully connected network or not
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')

# Half frozen the base model
base_model.trainable = True
fine_tune_at = 100
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable = False

# because in this case, I would have 4*4*1280(20480) new weights to train. So, to reduce input shape I use GlobalAveragePooling
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
prediction_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(global_average_layer)
model = tf.keras.models.Model(inputs=base_model.input, outputs=prediction_layer)

model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

data_gen_train = ImageDataGenerator(rescale=1/255.)
data_gen_valid = ImageDataGenerator(rescale=1/255.)

train_generator = data_gen_train.flow_from_directory(train_dir, target_size=(128,128), batch_size=128, class_mode='binary')
valid_generator = data_gen_valid.flow_from_directory(validation_dir, target_size=(128,128), batch_size=128, class_mode='binary')

model.fit_generator(train_generator, epochs=5, validation_data=valid_generator)
valid_loss, valid_accuracy = model.evaluate_generator(valid_generator)

print('Validation accuracy after fine tuning: {}'.format(valid_accuracy))
```

<br><br><br><br>

## Result
---
<br>

**Transfer Learning**
```
Accuracy after transfer learning: 0.9629999995231628
```
<br><br>

**Fine Tuning**
```
Validation accuracy after fine tuning: 0.9729999899864197
```
<br><br><br><br>

# Implementation

+ [Transfer Leraning Fine Tuning 1](https://github.com/csh970605/Complete-Guide-on-TensorFlow-2.0/tree/main/Section%206)

+ [Transfer Leraning Fine Tuning for image classification 1](https://github.com/csh970605/TensorFlow-2.0-Advanced/tree/main/Section%202)
+ [Transfer Leraning Fine Tuning for image classification 2](https://github.com/csh970605/Computer-Vision-Masterclass/tree/main/Section%206)