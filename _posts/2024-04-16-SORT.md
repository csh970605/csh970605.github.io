---
title: SORT
author: SeHoon
date: 2024-04-16 10:00:30 +0900
categories: [Papers, DeepLearning]
tags: [Papers, DeepLearning, MultiObjectTracking]
math: true
mermaid: true
---

# Simple Online and Realtime Tracking(SORT)

[SORT](https://arxiv.org/abs/1602.00763) is a novel multi object tracking algorithm that disunite detector and assigner unlike other traditional tracking algorithms.<br>
There is one more thing that different from other traditional tracking algorithms, they used simple but efficient methods which are [Kalman filter](https://csh970605.github.io/posts/Kalman_Filter/) and [Hungarian method](https://csh970605.github.io/posts/Hungarian_Method/) to handle the motion prediction and data association components of the tracking problems.<br>
And also, as the name suggests (Online Tracking), SORT is a method of performing detection response association to form a tracking trajectory using only object detection information of past and current frames without information about future frames.<br>
<br><br><br><br>


# Methodologies

The proposed method is described by the key components of:

+ Detection

+ Propagating object states into future frames(Estimation Model)

+ Associating current detections with existing objects

+ Managing the lifespan of tracked objects

<br><br>

## Detection

As I mentioned in the first paragraph, they used detection frameworks. It might be strange since it was made a long time ago, but at that time, using VGG16 was the best according to their experiences.<br>
We can see the comparsion of tracking performance by switching the detectors:

<center>

<img src='https://github.com/csh970605/csh970605.github.io/assets/28240052/83698201-b777-4f7b-9273-e837e4f8d2b8'><br>

</center>
<br><br>

## Estimation Model

To achieve the online tracking, they designed that a representation and motion model that is used to propagate a target's identity into the next frame.<br>
They calculated the inter-frame displacements of each object with a linear constant velocity model that is independent of othe objects and camera motion.<br>
The state of each target is:

<center>

$x = [u, v, s, r, \dot{u}, \dot{v}, \dot{s}]^T$
</center>
where 

+ $u$ : Horizontal pixel location of the center of the target.

+ $v$ : Vertical pixel location of the center of the target.

+ $s$ : Scale of the target's bounding box.

+ $r$ : Aspect ratio of the target's bounding box.

+ $\dot{u}$ : Future horizontal pixel location of the center of the target.

+ $\dot{v}$ : Future vertical pixel location of the center of the target.

+ $\dot{s}$ : Future scale of the target's bounding box.

    â€»Note that *r* is considered to be constant.

<br><br>


