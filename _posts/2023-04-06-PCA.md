---
title: Principal Component Analysis (PCA)
author: SeHoon
date: 2023-04-06 21:18:00 +0900
categories: [Machine Learning, ML_Theory]
tags: [machine learning, python]
math: true
mermaid: true
---

# What is PCA?

PCA is one of the most used unsupervised algorithms and dimensionality reduction algorithms. PCA reduces the dimensions of a D-dimensional data set by projecting onto a K-dimensional subspace where $K < D$.As PCA reduces the dimensions, it is important to select an axis that preserves the maximum variance because when the variance is large, the differences between the data become clear, which can make the model better, like image below.

<center>
<img src="https://github.com/csh970605/csh970605.github.io/assets/28240052/b11edef5-0f53-4abf-9239-2513e9097b28" width=600>
</center>
<br><br>
So PCA finds the axis with the maximum variance, and then finds the second axis orthogonal to this first axis and maximally preserving the remaining variance.<br>
The unit vector defining the i-th axis is called the i-th principal component (PC). In the example above, the first PC is c1 and the second PC is c2.


<br><br>

PCA is used for:
+ Noise filtering<br>
+ Visualization<br>
+ Feature Extraction<br>
+ Stock market predictions<br>
+ Gene data analysis<br>

The goal of PCA is:
+ Identify patterns in data.<br>
+ Detect the correlation between variables.<br>

The role of PCA is:
+ Standardize the data.
+ Obtain the eigenvectors and eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition<br>
+ Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace.<br>
+ Construct the projection matrix W from the selected $k$ eigen vectors.<br>
+ Transform the original data set X via W to obtain a k-dimensional feature subspace Y.<br>

But PCA has a weakness. PCA is highly effected by data outliers.

PCA can be expressed as:

<center>
<img src="https://user-images.githubusercontent.com/28240052/230391616-7843169d-5b38-4493-85d1-d41c16270fbe.png" width=400>
</center>

<br><br><br><br>

## Steps of PCA
---
<br>

1. Mean centering : The way to get $\tilde{X}$ which set the average of the random variables(One point) in the X matrix to 0.<br>

    + Calculate the average of each column(data of one random variable) of the X matrix.<br>
        $\bar{x_{i}} = \frac{1}{n}(x_{1,i}\ +\ x_{2,i}\ +\ \cdots\ +\ x_{n,i})$

    + For each column, subtract the (column's) mean from the data. This matrix is called the centered matrix $\tilde{X}$.<br>
        $\tilde{X}\ =\ X\ -\ \begin{bmatrix} \bar{x_{1}}&\bar{x_{2}}&\cdots&\bar{x_{i}}\\ \vdots&&&\vdots\\ \bar{x_{1}}&\bar{x_{2}}&\cdots&\bar{x_{i}}\\ \end{bmatrix}\ =\ \begin{bmatrix} x_{1,1}&x_{1.2}&\cdots&x_{1,i}\\ \vdots&&&\vdots\\ x_{n,1}&x_{n,2}&\cdots&x_{n,i}\\ \end{bmatrix}\ -\ \begin{bmatrix} \bar{x_{1}}&\bar{x_{2}}&\cdots&\bar{x_{i}}\\ \vdots&&&\vdots\\ \bar{x_{1}}&\bar{x_{2}}&\cdots&\bar{x_{i}}\\ \end{bmatrix}$
    <br><br>

2. Get SVD(Singular-value decomposition) of $\tilde{X}$<br>

    $\tilde{X} = U \Sigma V^{T}$ 
    where
    $U\ :\ i \times i\ orthogonal\ matrix\ which\ is\ eigen-vector\ of\ AA^{T}$<br>
    $\Sigma\ :\ m \times n\ rectangular\ diagonal\ matrix\ $<br>
    $V\ :\ n \times n\ orthogonal\ matrix\ which\ is\ eigen-vector\ of\ A^{T}A$<br>

    In this stage, we can get principal axes which is V.

3. Get PC score

    PC score = US in step2.

# Example
<br><br>

## Code
---
<br>

```py
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

pca = PCA(n_components=2)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
```

<br><br>

## Result
---
<br>

<center>
<img src="https://user-images.githubusercontent.com/28240052/230381397-89808268-7f3d-4c17-b8be-988ac5329c4a.png">
</center>

<br><br><br><br>

# Implementation

+ [PCA](https://github.com/csh970605/Machine-LearningA-Z/tree/main/Part%209%20-%20Dimensionality%20Reduction/Section%2043%20-%20Principal%20Component%20Analysis%20(PCA)/Python)