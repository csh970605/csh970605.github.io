---
title: Twin Delayed DDPG(TD3)
author: SeHoon
date: 2023-04-27 22:33:30 +0900
categories: [Deep Reinforcement Learning, DRL_Introduction]
tags: [deep reinforcement learning, python]
math: true
mermaid: true
---

# What is Twin Delayd DDPG(TD3)?

TD3 is one of the deep [reinforcement learning](https://csh970605.github.io/posts/Reinforcement_Learning/).<br>
TD3 double learns with one optimal value, and the doulbe learning includes two [actor models](https://csh970605.github.io/posts/Actor_Critic/) and four [critic models](https://csh970605.github.io/posts/Actor_Critic/). And also [policy gradient](https://csh970605.github.io/posts/Policy_Gradient/) is used to update actor model through the [Q values](https://csh970605.github.io/posts/Q_Learning/) optimized by critic model.
<br>
The basic shape of TD3 is:

<center>
<img src="https://user-images.githubusercontent.com/28240052/235915907-4c133354-3fd8-4141-b495-2bbada1bca32.png" width=800>
</center>
<br><br>
And the steps of weights updating are actor target &rarr; critic targets &rarr; critic models &rarr; actor model

<br>
<br>
<br>

# Stpes of TD3
<br>

+ Step 1<br>
Initialize the **Experience Replay Memory** that stores past trasitions from which we are going to learn our Q values.<br>
And the Experience Replay Memory is consisted of:<br>
```
   s  : current state
   s' : next state
   a  : action played, it leads s to s'
   r  : reward
```

<center>
<img src="https://user-images.githubusercontent.com/28240052/235912996-969b724e-324d-4bff-9f9a-52f51db1e92c.png" width=500>
</center>
<br><br>

+ Step 2<br>
Build one neural network fro the actor model and one neural network for actor target.
<center>
<img src="https://user-images.githubusercontent.com/28240052/235914517-9e24b309-33f5-4695-a345-32c2232e059c.png" width=700>
</center>
<br><br>

<center>
<img src="https://user-images.githubusercontent.com/28240052/235915104-7bea795e-258f-4fff-98c2-6fdcfeee9975.png" width=700>
</center>
<br><br>

+ Step 3<br>
Build two neural networks for the two Critic models and two neural networks for the two Critic targets.
<center>
<img src="https://user-images.githubusercontent.com/28240052/235916575-1a076a65-58b6-4043-9e62-af85c31d7db6.png" width=800>
</center>
<br><br>


+ Step 4(Training process)<br>
Run a full episode with certain numbers of actions randomly to avoid to end up in a bad state. And then with actions played by the actor model.
<br>
<br>

+ Step 5<br>
Sample a batch of transitions(s, s', a, r) from the experience replay memory. Then for each element of the batch:
<center>
<img src="https://user-images.githubusercontent.com/28240052/235934181-9d17aedd-7102-456f-8b51-81e30ea6b9ef.png" width=800>
</center>
<br><br>

+ Step 6<br>
From the next state(s') the actor target plays the next action a'.
<center>
<img src="https://user-images.githubusercontent.com/28240052/235934904-d785970a-97f3-45e5-9d66-c2332ea00514.png" width=800>
</center>
<br><br>

+ Step 7<br>
Add gaussian noise to next action(a') and clamp it in a range of values supported by the environment in order to make two actions different. This process will get us better state or avoid the agent from being stuck in a state.<br>
And the formula of gaussian noise is:
<center>
<font size=4>

$ \tilde{a}\ \gets\ \pi_{\phi'}(s')\ +\ \epsilon,\ \epsilon\ \sim\ clip(N(0, \tilde{a}), -c, c) $
<br>

$ \tilde{a}\ \gets\ clip(\tilde{a}) $

</font>
</center>
<br><br>

+ Step 8<br>
The two critic targets take each the couple(s', a') as input and return two Q values $Q_{t1}(s',a') $ and $Q_{t2}(s',a') $ as output.

<center>
<img src="https://user-images.githubusercontent.com/28240052/235938870-8f4ee2db-1113-4e97-8502-baedaf466575.png" width=800>
</center>
<br><br>

+ Step 9<br>
Keep the minimum of two Q values($min(Q_{t1},\ Q_{t2})$) to prevent too optimistic estimates of that value of state. That is, it helps to stabilize the training process.<br>
It represents the approximated value of the next state.
<center>
<img src="https://user-images.githubusercontent.com/28240052/235941234-2e411264-13e0-45e1-8336-99ff6d3ded60.png" width=500>
</center>
<br><br>

+ Step 6<br>
<br><br>

+ Step 6<br>
<br><br>

+ Step 6<br>
<br><br>


<br>
<br>
<br>

# Exmaple Code<br>

```py
```

<br><br><br>

## Result
---
<br>
<img src="https://drive.google.com/uc?export=view&id=1ODPTkc7q-Iap2969bwkJMjluPm5iDRCR"><br>

<center>
<img src="" width=500>
</center>
<br><br>